name: TextVQA
type: V-logit
loss: TextVQA
metric: TextVQA
process: normal
task_id: 19

# TextVQA holders and folders
tvqa_dataroot: datasets/textvqa
tvqa_obj_features: "/srv/share/ykant3/vilbert-mt/features/obj/{}_obj.lmdb"
tvqa_ocr_features: "/srv/share/ykant3/vilbert-mt/features/ocr/{}_ocr.lmdb"
tvqa_imdb: "/srv/testing/ykant3/pythia/imdb/textvqa_0.5/textvqa_{}.npy"

# ST-VQA holders and folders
stvqa_dataroot: datasets/stvqa
stvqa_obj_features: "/srv/share/ykant3/scene-text/features/obj/lmdbs/train_task_fixed.lmdb"
stvqa_ocr_features: "/srv/share/ykant3/scene-text/features/ocr/lmdbs/train_task_fixed.lmdb"
stvqa_imdb: "/nethome/ykant3/pythia/data/imdb/textvqa_0.5/stvqa_{}.npy"

max_seq_length: 20
max_obj_num: 100 # object features + avg feature
max_ocr_num: 50 # ocr tokens + avg feature
batch_size: 96
train_split: train
val_split: val
lr: 0.0001
num_epoch: 100
max_region_num: -1 # backward compatibility
debug: false
grad_clip_mode: all
max_grad_norm: 0.25
model_type: m4c_spatial
lr_scheduler: pythia_warmup_decay
optim: Adam
lr_decay_iters: [14000, 19000]
lr_decay: 0.1
warmup_factor: 0.2
warmup_iters: 1000
vocab_type: 5k
heads_type: mix
train_on: ["textvqa"]
val_on: ["textvqa"]
test_on: ["textvqa"]
num_workers: 16
needs_spatial: true
layer_type_list: [n,n,s,s,s,s]
mix_list: [none, none, share3, share3, share3, share3]

M4C:
  obj_drop: 0.1
  ocr_drop: 0.1
  hidden_size: 768
  num_hidden_layers: 2
  num_spatial_layers: 4
  num_spatial_relations: 12
  type_vocab_size: 2
  vocab_size: 30522
  textvqa_vocab_size: 3998
  pooling_method: "mul"
  ptr_query_size: 768
  ocr_feature_size: 3002
  obj_feature_size: 2048
  finetune_ocr_obj: false
  use_phoc_fasttext: true
  normalize: true
  lr_scale_mmt: 1.0
  num_decoding_steps: 12
  max_obj_num: 100
  max_ocr_num: 50
  max_seq_length: 20
  #  Quadrants (QUE + OCR and OBJ + DEC)
  #  1 | 2 | 3
  #  ---------
  #  4 | 5 | 6
  #  ---------
  #  7 | 8 | 9
  attention_mask_quadrants: [1,2]
  beam_size: 1

TextBert:
  lr_scale_text_bert: 0.1
  num_hidden_layers: 4
  text_bert_init_from_bert_base: true
  vocab_size: 30522
